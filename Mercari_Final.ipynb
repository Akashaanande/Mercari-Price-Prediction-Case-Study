{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mercari_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RcIn0XDwCUZ1",
        "04WsPvRFITdi",
        "df6EHjirN2Me",
        "6B5ncFCkREO7",
        "EX4qyYC0RLZj"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcIn0XDwCUZ1"
      },
      "source": [
        "# **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMhMC8gfCSde",
        "outputId": "54075ebc-c64f-4633-9c1d-c154b337b818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tk4mbEJCBKQ"
      },
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from scipy import sparse\n",
        "from scipy.sparse import hstack\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import math\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from IPython.display import Image\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "import math\n",
        "from lightgbm import LGBMRegressor\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input , Dropout, Flatten,concatenate,LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard,ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input , Dropout, Flatten,concatenate,LSTM"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04WsPvRFITdi"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AC7gvDqJX9R"
      },
      "source": [
        "def data_cleaning(df):\n",
        "\n",
        "  df['category_name']=np.where(df['category_name'].isnull(),'Unknown_Category',df['category_name'])\n",
        "  df['brand_name']=np.where(df['brand_name'].isnull(),'Unknown_Brand',df['brand_name'])\n",
        "  df['item_description']=np.where(df['item_description'].isnull(),'Unknown_description',df['item_description'])\n",
        "\n",
        "  df3 =df['category_name'].str.split('/',expand=True)\n",
        "  df3.columns = ['category{}'.format(x+1) for x in df3.columns]\n",
        "  l=df3.columns.tolist()\n",
        "  while len(l)!=5:\n",
        "    l.append('category{}'.format(len(l)+1))\n",
        "    \n",
        "  for j in l:\n",
        "    for i in df3.columns:\n",
        "      if j not in df3:\n",
        "        df3[j]=np.nan\n",
        "    \n",
        "  df = df.join(df3)  \n",
        "\n",
        "  df['category1']=np.where(df['category1'].isnull(),'undefined',df['category1'])\n",
        "  df['category2']=np.where(df['category2'].isnull(),'undefined',df['category2'])\n",
        "  df['category3']=np.where(df['category3'].isnull(),'undefined',df['category3'])\n",
        "  df['category4']=np.where(df['category4'].isnull(),'undefined',df['category4'])\n",
        "  df['category5']=np.where(df['category5'].isnull(),'undefined',df['category5'])\n",
        "\n",
        "  return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWXUIK0lJYqR"
      },
      "source": [
        "# https://stackoverflow.com/a/47091490/4084039\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzs9_5yoKWot"
      },
      "source": [
        "# https://gist.github.com/sebleier/554280\n",
        "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
        "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBKbkBg_KW1A"
      },
      "source": [
        "def preprocess_txt(data):\n",
        "    preprocessed = []\n",
        "    # tqdm is for printing the status bar\n",
        "    for sentance in data:\n",
        "        sent = decontracted(sentance)\n",
        "        sent = sent.replace('\\\\r', ' ')\n",
        "        sent = sent.replace('\\\\\"', ' ')\n",
        "        sent = sent.replace('\\\\n', ' ')\n",
        "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "        # https://gist.github.com/sebleier/554280\n",
        "        sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
        "        preprocessed.append(sent.lower().strip())\n",
        "    return preprocessed"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb3z2xHDKmP8"
      },
      "source": [
        "def basic_text_features(df,column,stopwords):\n",
        "\n",
        "  df[column]=df[column].astype(str)\n",
        "\n",
        "  df[\"No_of_words\"+column]=df[column].apply(lambda x : len(str(x).split(\" \")))\n",
        "  df[\"No_of_chars\"+column]=df[column].str.len()\n",
        "  df['Avg_len_'+column]=df[column].apply(lambda x : (sum(len(word) for word in x.split())/len(x.split())))\n",
        "  df[\"No_of_stop_words\"+column]=df[column].apply(lambda x: len([x for x in x.split() if x in stopwords]))\n",
        "  df['No_of_digits'+column]=df[column].apply(lambda x : len([x for x in x.split() if x.isdigit()]))\n",
        "  df['No_of_upper_cased'+column]=df[column].apply(lambda x : len([x for x in x.split() if x.isupper()]))\n",
        "  df['No_of_lower_cased'+column]=df[column].apply(lambda x : len([x for x in x.split() if x.islower()]))\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njVgol8QLCrn"
      },
      "source": [
        "def encoding_categorical_variables(col,X_train,X_test,df):\n",
        "  \n",
        "  \"\"\" One hot Encode Categorical Variables using countvectorizer \"\"\"\n",
        "  \n",
        "  from collections import Counter\n",
        "  my_counter = Counter()\n",
        "  for word in df[col].astype(str).values:\n",
        "      my_counter.update(word.split())\n",
        "\n",
        "  cat_dict = dict(my_counter)\n",
        "  sorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n",
        "\n",
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "  vectorizer = CountVectorizer(vocabulary=list(sorted_cat_dict.keys()), lowercase=False, binary=True)\n",
        "  X_train_cat = vectorizer.transform(X_train[col].values)\n",
        "  X_test_cat = vectorizer.transform(X_test[col].values)\n",
        "  \n",
        "  return X_train_cat ,X_test_cat"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z-2Epp0LDF4"
      },
      "source": [
        "def scaling_numerical_features(col,df_train,df_test):\n",
        "  \n",
        "  \"\"\" Normalizin Numerical Variables \"\"\"\n",
        "\n",
        "  from sklearn.preprocessing import Normalizer\n",
        "  normalizer = Normalizer()\n",
        "  normalizer.fit(df_train[col].values.reshape(-1,1))\n",
        "\n",
        "  train_norm = normalizer.transform(df_train[col].values.reshape(-1,1))\n",
        "  test_norm = normalizer.transform(df_test[col].values.reshape(-1,1))\n",
        "\n",
        "  return train_norm , test_norm"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1idAkXXLDIf"
      },
      "source": [
        "def encoding_numerical_categories(col,X_train,X_test):\n",
        "  \"\"\" Encoding Numerical Categories \"\"\"\n",
        "\n",
        "  from sklearn.preprocessing import OneHotEncoder\n",
        "  enc = OneHotEncoder()\n",
        "  enc.fit(X_train[col].values.reshape(-1, 1))\n",
        "  X_train_enc = enc.transform(X_train[col].values.reshape(-1, 1))\n",
        "  X_test_enc = enc.transform(X_test[col].values.reshape(-1, 1))\n",
        "\n",
        "  return X_train_enc,X_test_enc"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df6EHjirN2Me"
      },
      "source": [
        "# **Define functions for Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUg4BONGQL4R"
      },
      "source": [
        "def mlp_model3(dim):\n",
        "\n",
        "  Input_layer=Input(shape=(dim.shape[1],))\n",
        "  Input_lay=Dense(2046, activation='relu')(Input_layer)\n",
        "  L1=Dense(1024, activation='relu')(Input_lay)\n",
        "  L2=Dense(512, activation='relu')(L1)\n",
        "  L3=Dense(256, activation='relu')(L2)\n",
        "  L4=Dense(128, activation='relu')(L3)\n",
        "  L5=Dense(64, activation='relu')(L4)\n",
        "  L6=Dense(32, activation='relu')(L5)\n",
        "  out=Dense(1)(L6)\n",
        "  model=Model(Input_layer,out)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfZPSC3WmQ6Y"
      },
      "source": [
        "def mlp_model4(dim):\n",
        " \n",
        "  Input_layer=Input(shape=(dim.shape[1],))\n",
        "  #Input_lay=Dense(2046, activation='relu')(Input_layer)\n",
        "  L1=Dense(1024, activation='relu')(Input_layer)\n",
        "  L2=Dense(512, activation='relu')(L1)\n",
        "  L3=Dense(256, activation='relu')(L2)\n",
        "  L4=Dense(128, activation='relu')(L3)\n",
        "  L5=Dense(64, activation='relu')(L4)\n",
        "  L6=Dense(32, activation='relu')(L5)\n",
        "  out=Dense(1)(L6)\n",
        "  model=Model(Input_layer,out)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGEDHyaFKyiP"
      },
      "source": [
        "def MLP_Models(mlp3,mlp4,X_te_tfidf):\n",
        "  \"\"\" Get predictions fromMLP Models \"\"\"\n",
        "  model = mlp3\n",
        "  model.load_weights('/content/drive/My Drive/Kaggle/Mercari/approach_1/DL_Models/model-001-0.163283-0.163283.h5')\n",
        "  preds = model.predict(X_te_tfidf,batch_size = 512,verbose = 1)[:, 0]\n",
        "  preds_mlp3=np.expm1((preds.reshape(-1, 1))[:, 0])\n",
        "\n",
        "  model = mlp4\n",
        "  model.load_weights('/content/drive/My Drive/Kaggle/Mercari/approach_1/DL_Models/model-001-0.162473-0.162473.h5')\n",
        "  preds = model.predict(X_te_tfidf,batch_size = 200,verbose = 1)[:, 0]\n",
        "  preds_mlp4=np.expm1((preds.reshape(-1, 1))[:, 0])\n",
        "\n",
        "  pred_final_mlp=(0.510204*preds_mlp4) + ((1-0.510204)*preds_mlp3)\n",
        "\n",
        "  return pred_final_mlp"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V0HezKURQqq"
      },
      "source": [
        "def LGBM_models(X_te_tfidf):\n",
        "  LGBM_1 = pickle.load(open('/content/drive/My Drive/Kaggle/Mercari/approach_1/DL_Models/LGBM_best15000.pkl',\"rb\"))\n",
        "  preds_LGBM_1=np.expm1(LGBM_1.predict(X_te_tfidf))\n",
        "  LGBM_2 = pickle.load(open('/content/drive/My Drive/Kaggle/Mercari/approach_1/DL_Models/LGBM_best10000.pkl',\"rb\"))\n",
        "  preds_LGBM_2=np.expm1(LGBM_2.predict(X_te_tfidf))\n",
        "\n",
        "  pred_final_LGBM=(1.000000*preds_LGBM_1) + ((1-1.000000)*preds_LGBM_2)\n",
        "\n",
        "  return pred_final_LGBM"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B5ncFCkREO7"
      },
      "source": [
        "# **Function 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcsUIQ2mLDLK"
      },
      "source": [
        "def function1(X):\n",
        "\n",
        "  ########################### Data Cleaning #######################################\n",
        "\n",
        "  X_inp= data_cleaning(X)\n",
        "  X_inp['product_name_preprocessed']=preprocess_txt(X_inp['name'])\n",
        "  X_inp['item_description_preprocessed']=preprocess_txt(X_inp['item_description'].astype(str))\n",
        "  stop=stopwords\n",
        "  X_inp=basic_text_features(X_inp,\"item_description\",stop)\n",
        "  X_inp=basic_text_features(X_inp,\"name\",stop)\n",
        "  X_test=X_inp\n",
        "\n",
        "  X= pd.read_pickle('/content/drive/My Drive/Kaggle/Mercari/approach_1/X.pkl')\n",
        "  X_train=pd.read_pickle('/content/drive/My Drive/Kaggle/Mercari/approach_1/X_train.pkl')\n",
        "\n",
        "  #################### Preprocessing and Encoding ###################################################\n",
        "  X_train_brand_name,X_test_brand_name=encoding_categorical_variables('brand_name',X_train,X_test,X)\n",
        "  X_train_category1,X_test_category1=encoding_categorical_variables('category1',X_train,X_test,X)\n",
        "  X_train_category2,X_test_category2=encoding_categorical_variables('category2',X_train,X_test,X)\n",
        "  X_train_category3,X_test_category3=encoding_categorical_variables('category3',X_train,X_test,X)\n",
        "  X_train_category4,X_test_category4=encoding_categorical_variables('category4',X_train,X_test,X)\n",
        "  X_train_category5,X_test_category5=encoding_categorical_variables('category5',X_train,X_test,X)\n",
        "\n",
        "  \n",
        "\n",
        "  X_train_wordsitem_description,X_test_wordsitem_description=scaling_numerical_features('No_of_wordsitem_description',X_train,X_test)\n",
        "  X_train_charsitem_description,X_test_charsitem_description=scaling_numerical_features('No_of_charsitem_description',X_train,X_test)\n",
        "  X_train_len_item_description,X_test_len_item_description=scaling_numerical_features('Avg_len_item_description',X_train,X_test)\n",
        "  X_train_stop_wordsitem_description,X_test_stop_wordsitem_description=scaling_numerical_features('No_of_stop_wordsitem_description',X_train,X_test)\n",
        "  X_train_digitsitem_description,X_test_digitsitem_description=scaling_numerical_features('No_of_digitsitem_description',X_train,X_test)\n",
        "  X_train_upper_caseditem_description,X_test_upper_caseditem_description=scaling_numerical_features('No_of_upper_caseditem_description',X_train,X_test)\n",
        "  X_train_lower_caseditem_description,X_test_lower_caseditem_description=scaling_numerical_features('No_of_lower_caseditem_description',X_train,X_test)\n",
        "\n",
        "  X_train_wordsname,X_test_wordsname=scaling_numerical_features('No_of_wordsname',X_train,X_test)\n",
        "  X_train_charsname,X_test_charsname=scaling_numerical_features('No_of_charsname',X_train,X_test)\n",
        "  X_train_len_name,X_test_len_name=scaling_numerical_features('Avg_len_name',X_train,X_test)\n",
        "  X_train_stop_wordsname,X_test_stop_wordsname=scaling_numerical_features('No_of_stop_wordsname',X_train,X_test)\n",
        "  X_train_digitsname,X_test_digitsname=scaling_numerical_features('No_of_digitsname',X_train,X_test)\n",
        "  X_train_upper_casedname,X_test_upper_casedname=scaling_numerical_features('No_of_upper_casedname',X_train,X_test)\n",
        "  X_train_lower_casedname,X_test_upper_lower_casedname=scaling_numerical_features('No_of_lower_casedname',X_train,X_test)\n",
        "  X_train_enc_shipping,X_test_enc_shipping = encoding_numerical_categories('shipping',X_train,X_test)\n",
        "  X_train_enc_item_condition,X_test_enc_item_condition = encoding_numerical_categories('item_condition_id',X_train,X_test)\n",
        "\n",
        "  ############################## Vectorization ##############################################################################\n",
        "  vectorizer = TfidfVectorizer(max_features=100000)\n",
        "  vectorizer.fit(X_train['product_name_preprocessed'].values.astype('U')) \n",
        "\n",
        "  X_train_prod_name_tfidf = vectorizer.transform(X_train['product_name_preprocessed'].values.astype('U'))\n",
        "  X_test_prod_name_tfidf = vectorizer.transform(X_test['product_name_preprocessed'].values.astype('U'))\n",
        "\n",
        "  vectorizer = TfidfVectorizer(max_features=100000)\n",
        "  vectorizer.fit(X_train['item_description_preprocessed'].values.astype('U')) \n",
        "\n",
        "  X_train_item_desc_tfidf = vectorizer.transform(X_train['item_description_preprocessed'].values.astype('U'))\n",
        "  X_test_item_desc_tfidf = vectorizer.transform(X_test['item_description_preprocessed'].values.astype('U'))\n",
        "\n",
        "  ############################# Final Train Test Data #############################################################################\n",
        "  X_tr_tfidf = hstack((X_train_prod_name_tfidf,X_train_item_desc_tfidf,X_train_brand_name,X_train_category1,X_train_category2,X_train_category3,X_train_category4,X_train_category5,X_train_wordsitem_description,X_train_charsitem_description,\tX_train_len_item_description,X_train_stop_wordsitem_description,X_train_digitsitem_description,X_train_upper_caseditem_description,X_train_lower_caseditem_description,X_train_wordsname,X_train_charsname,X_train_len_name,X_train_stop_wordsname,X_train_digitsname,X_train_upper_casedname,X_train_lower_casedname,X_train_enc_shipping,X_train_enc_item_condition)).tocsr()\n",
        "  X_te_tfidf = hstack((X_test_prod_name_tfidf,X_test_item_desc_tfidf,X_test_brand_name,X_test_category1,X_test_category2,X_test_category3,X_test_category4,X_test_category5,X_test_charsitem_description,X_test_len_item_description,X_test_stop_wordsitem_description,X_test_digitsitem_description,X_test_upper_caseditem_description,X_test_lower_caseditem_description,X_test_wordsitem_description,X_test_wordsname,X_test_charsname,X_test_len_name,X_test_stop_wordsname,X_test_digitsname,X_test_upper_casedname,X_test_upper_lower_casedname,X_test_enc_shipping,X_test_enc_item_condition)).tocsr()\n",
        "\n",
        "\n",
        "  ################################## Prediction using trained Models #############################################################\n",
        "  model3=mlp_model3(X_tr_tfidf)\n",
        "  model4=mlp_model4(X_tr_tfidf)\n",
        "\n",
        "  pred_final_mlp=MLP_Models(model3,model4,X_te_tfidf)\n",
        "  pred_final_LGBM=LGBM_models(X_te_tfidf)\n",
        "  \n",
        "\n",
        "  pred_final_ensemble=(0.367347*pred_final_LGBM) + ((1-0.367347)*pred_final_mlp)\n",
        "\n",
        "  return pred_final_ensemble"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjOXVg6LQYW_"
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/Kaggle/Mercari/test_stg2.tsv',sep='\\t', header=0)\n",
        "X_Input=df[2:3]\n",
        "X_Input.set_index('test_id',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcdY7Py0ZkhO",
        "outputId": "e2022644-c5e1-4592-e0d1-592550967033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "pred=function1(X_Input)\n",
        "print('The predicted price for product --- >',X_Input.values,'is--->',pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "The predicted price for product --- > [['Coach bag' 1 'Vintage & Collectibles/Bags and Purses/Handbag' 'Coach'\n",
            "  1 'Brand new coach bag. Bought for [rm] at a Coach outlet.']] is---> [60.11525188]\n",
            "CPU times: user 1min 46s, sys: 5.75 s, total: 1min 51s\n",
            "Wall time: 2min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX4qyYC0RLZj"
      },
      "source": [
        "# **Function2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykJ3xO0DRQNt"
      },
      "source": [
        "def rmsle_score(y, y_pred):\n",
        "    assert len(y) == len(y_pred)\n",
        "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
        "    return (sum(to_sum) * (1.0/len(y))) ** 0.5"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ZQfYmBRQQU"
      },
      "source": [
        "def function2(X,Y):\n",
        "  score=rmsle_score(Y,function1(X))\n",
        "  return score"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a63DSrC1SKXU"
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/Kaggle/Mercari/test_stg2.tsv',sep='\\t', header=0)\n",
        "X_Input=df[2:3]\n",
        "price=[70]\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP-6w_HIOGaE",
        "outputId": "033fa571-e542-4a57-e3f7-ecb1a9906a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "score=function2(X_Input,price)\n",
        "print('The score --- >is',score)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "The score --- >is 0.14991823311764385\n",
            "CPU times: user 1min 47s, sys: 6.81 s, total: 1min 54s\n",
            "Wall time: 2min 41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0eX3PyvxSle"
      },
      "source": [
        "df=pd.read_pickle('/content/drive/My Drive/Kaggle/Mercari/approach_1/X_test.pkl')\n",
        "df.drop(['category1', 'category2', 'category3', 'category4', 'category5'], axis=1, inplace=True)\n",
        "X_Input=df[2:3]\n",
        "price=X_Input['price'].tolist()\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR-llhEJxS37",
        "outputId": "ea1c8478-8478-4a87-ec04-7b840bbd29aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "score=function2(X_Input,price)\n",
        "print('The score --- >is',score)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ef75874e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ef75874ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 1ms/step\n",
            "The score --- >is 0.1837494319053734\n",
            "CPU times: user 1min 49s, sys: 4.6 s, total: 1min 54s\n",
            "Wall time: 1min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}